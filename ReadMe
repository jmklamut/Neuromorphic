Spiking neural networks (SNNs), as brain-inspired neural networks, have received significant attention due to their advantages of low power consumption, high parallelism, and high fault tolerance. However, compared to artificial neural networks (ANNs), their performance is relatively low. To address this issue, we propose an SNN with Transformer-based frame-event fusion for recognition, specifically focusing on classifying different incoming sound waves to identify their source. Unlike traditional methods, which often struggle to represent the rich and complex structures of audio signals, Transformers can process the entire input sequence simultaneously using self-attention, making them more efficient in handling temporal relationships in audio data. For better feature extraction and computational efficiency, we employ a hybrid approach where the Transformer is primarily used to model global dependencies, while the SNN handles the remaining operations. 

The goal of this project is to design an SNN model with a Transformer for recognition, specifically focusing on audio detection and classification. When an input audio sound wave is detected by the model, it will be processed and analyzed. Finally, the model will classify it to the best of its ability. 

The methods used for this project will be described below.  

SNNs are biological neural systems and use spiking signals to process information. This spiking resembles the brain in biology in how biological neurons fire spikes. For our project, we will use the LIF model. The LIF model generates a spike when the input current exceeds a certain threshold. When the input current surpasses leakage current, the membrane potential increases. Once this membrane potential exceeds a threshold, a spike is released as output.  

A spiking transformer is like a spiking neural network however it utilizes the powerful sequential modeling capabilities of transformers. Unlike traditional transformer that use continuous activations, spiking transformers process information as discrete spike events over time. The input is first encoded into spike trains to represent data that mimic neural firing. Like general transformers, spiking transformers leverage self-attention to model dependencies. However, these are modified to work with the spike driven computation.  

Firstly, the input layer will handle audio input. For the audio preprocessing, we can apply methods such as log-mel methods to convert waveforms into log-mel spectrograms. These extracted features will be fed into our spiking transformer. The output of the spiking transformer will be recognition classes. In terms or audio sources, these classes can be drilling noises, car noises, and other noises that are provided from the publicly available data set called UrbanSound8k-AV. 

For our results, we expect to have a fully working spiking transformer that can perform recognition, specifically audio files. The performance of our model will vary depending on training stabilization along with potential tradeoffs with accuracy and latency. Additionally, the dataset plays a role in this as well. In the end, our goal is to have the model perform recognition task with high accuracy, however this accuracy is second to creating a functional spiking transformer for audio recognition due time constraints. Additionally, if time allows, we will add visual recognition alongside audio recognition in our spiking transformer. 
